Train_Validate:


  # Paths / data
  data_path: "/app/data/tutorial_dataset"
  log_dir: "/app/work_dir/tutorial_dataset/logs/"
  train_csv_name: "/app/data/catalog_2023.csv"


  # Dataset / normalization
  data_size: 224
  buffer: 0
  buffer_comp: 0
  global_stats: null
  catalog_index: null
  trans: null
  parallel: null

  img_path_cols: ["image"]
  apply_normalization: true
  normal_strategy: "min_max"
  stat_procedure: "lab"
  downfactor: 32
  clip_val: 0
  nodata: []


  # Data augmentation

  scale_factor: [0.75, 1.5]
  crop_strategy: "center"
  rotation_degree: [-180, -90, 90, 180]
  sigma_range: [0.03, 0.07]
  br_range: [-0.02, 0.02]
  contrast_range: [0.9, 1.2]
  bshift_gamma_range: [0.2, 2.0]
  patch_shift: true


  # Batching

  train_batch: 40
  validate_batch: 20


  # Diffusion noise schedule ONLY

  diffusion:
    timesteps: 1000
    noise_schedule: "linear"     # or "linear"
    beta_start: 1e-6
    beta_end: 0.02

  # Model

  model: "unet_att_d"
  channels: 4
  save_dir: "/app/work_dir/exp_pretrain_100k"

  model_kwargs:
    filter_config: [64, 256, 512, 1024, 2048, 4096]
    block_num: [2, 2, 2, 2, 2, 2]
    dropout_rate: 0.1
    dropout_type: "traditional"
    upmode: "deconv_2"
    use_skipAtt: true
    time_embedding_dim: 128


  # Training

  epoch: 100

  # Optimizer (AdamW only)
  optimizer: "adamw"
  learning_rate_init: 2e-4
  weight_decay: 1e-4
  # momentum/rho/adaptive are not used for AdamW (SAM-only), so omitted
  betas: [0.9, 0.999]
  eps: 1e-8

  # LR Scheduler (choose ONE)
  scheduler: "cosinewarm"        # options: cosinewarm | cosine | step | multistep | plateau | poly | onecycle
  t_0: 30 #10                       # CosineAnnealingWarmRestarts
  t_mult: 2
  eta_min: 1e-5 #5e-6

  # Examples (COMMENTED):
  # scheduler: "cosine"          # CosineAnnealingLR
  # T_max: 50
  # eta_min: 1e-6
  #
  # scheduler: "step"            # StepLR
  # step_size: 20
  # gamma: 0.5
  #
  # scheduler: "plateau"         # ReduceLROnPlateau
  # patience: 5
  # factor: 0.5
  # min_lr: 1e-6
  #
  # scheduler: "poly"            # PolynomialLR (linear-ish if power=1)
  # max_decay_steps: 100
  # min_learning_rate: 5e-6
  # power: 1.0


  # Loss

  loss_fn: "mse"                  # mse | l1 | charbonnier | ssim | perceptual
  loss_weights:
    alpha: 0.9
    beta: 0.1
    gamma: 0.0


  # Checkpointing / misc

  checkpoint_interval: 1
  warmup_period: 5
